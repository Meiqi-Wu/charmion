{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a24a0ec90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from moa.model import DenseNet, Model, DenseBlock\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_dir = './DATA/lish-moa/'\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(data_dir+'train_features.csv', index_col='sig_id')\n",
    "y = pd.read_csv(data_dir+'train_targets_scored.csv', index_col='sig_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for cp_type and cp_dose\n",
    "X['cp_type'].replace({'trt_cp':1., 'ctl_vehicle':0.}, inplace=True)\n",
    "X['cp_dose'].replace({'D1':1., 'D2':0.}, inplace=True)\n",
    "\n",
    "# # split into training set and test set\n",
    "# ids = X.index.values.copy()\n",
    "# np.random.shuffle(ids)\n",
    "\n",
    "# train_perc, test_perc = 0.85, 0.15\n",
    "# train_id = ids[:round(len(ids)*train_perc)]\n",
    "# test_id = ids[round(len(ids)*train_perc):]\n",
    "\n",
    "# X_train = X.loc[train_id]\n",
    "# X_test = X.loc[test_id]\n",
    "\n",
    "# y_train = y.loc[train_id]\n",
    "# y_test = y.loc[test_id]\n",
    "\n",
    "# # normalize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_norm = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "# X_train_norm.columns = X_train.columns\n",
    "# X_train_norm.index = X_train.index\n",
    "\n",
    "# X_test_norm = pd.DataFrame(scaler.transform(X_test))\n",
    "# X_test_norm.columns = X_test.columns\n",
    "# X_test_norm.index = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1, 40] : train loss 0.6575137376785278\n",
      "Epoch [1, 80] : train loss 0.3032241463661194\n",
      "Epoch [1, 120] : train loss 0.11416324228048325\n",
      "Validation loss decreased (inf --> 0.091764).  Saving model ...\n",
      "Epoch [2, 40] : train loss 0.07053825259208679\n",
      "Epoch [2, 80] : train loss 0.060192350298166275\n",
      "Epoch [2, 120] : train loss 0.053510840982198715\n",
      "Validation loss decreased (0.091764 --> 0.053594).  Saving model ...\n",
      "Epoch [3, 40] : train loss 0.0505717396736145\n",
      "Epoch [3, 80] : train loss 0.0464518740773201\n",
      "Epoch [3, 120] : train loss 0.04203011095523834\n",
      "Validation loss decreased (0.053594 --> 0.043874).  Saving model ...\n",
      "Epoch [4, 40] : train loss 0.042121246457099915\n",
      "Epoch [4, 80] : train loss 0.04004057124257088\n",
      "Epoch [4, 120] : train loss 0.03692217543721199\n",
      "Validation loss decreased (0.043874 --> 0.039242).  Saving model ...\n",
      "Epoch [5, 40] : train loss 0.038430675864219666\n",
      "Epoch [5, 80] : train loss 0.03667626902461052\n",
      "Epoch [5, 120] : train loss 0.034974079579114914\n",
      "Validation loss decreased (0.039242 --> 0.037358).  Saving model ...\n",
      "Epoch [6, 40] : train loss 0.03566981106996536\n",
      "Epoch [6, 80] : train loss 0.03432226926088333\n",
      "Epoch [6, 120] : train loss 0.032599009573459625\n",
      "Validation loss decreased (0.037358 --> 0.035298).  Saving model ...\n",
      "Epoch [7, 40] : train loss 0.034476038068532944\n",
      "Epoch [7, 80] : train loss 0.03389277309179306\n",
      "Epoch [7, 120] : train loss 0.03138948604464531\n",
      "Validation loss decreased (0.035298 --> 0.034881).  Saving model ...\n",
      "Epoch [8, 40] : train loss 0.034384287893772125\n",
      "Epoch [8, 80] : train loss 0.032314419746398926\n",
      "Epoch [8, 120] : train loss 0.05613025277853012\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch [9, 40] : train loss 0.04945298284292221\n",
      "Epoch [9, 80] : train loss 0.04679000377655029\n",
      "Epoch [9, 120] : train loss 0.04287915304303169\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [10, 40] : train loss 0.04455965757369995\n",
      "Epoch [10, 80] : train loss 0.04347407817840576\n",
      "Epoch [10, 120] : train loss 0.04139081761240959\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "params_net = {'input_size': X_train_norm.shape[1],\n",
    "             'hidden_size': [1024, 512, 1024],\n",
    "             'output_size': y_train.shape[1],\n",
    "             'dropout': 0.01}\n",
    "\n",
    "params_fit = {'X':X_train_norm,\n",
    "             'y': y_train,\n",
    "             'epoch': 20,\n",
    "             'lr': 5e-4,\n",
    "             'batch_size':128,\n",
    "             'L1': 1e-6,\n",
    "             'L2': 1e-6,\n",
    "             'pos_weight':1,\n",
    "             'patience':3,\n",
    "             'verbose':True}\n",
    "\n",
    "net = DenseNet(**params_net)\n",
    "model = Model(net)\n",
    "model.fit(**params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_proba(X_train_norm)\n",
    "y_test_pred = model.predict_proba(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009937683484609957"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_train.values.ravel(), y_train_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017457496457342073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test.values.reshape(-1), y_test_pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    }
   ],
   "source": [
    "n_SEED = 1\n",
    "n_fold = 5\n",
    "kfold = KFold(n_splits=n_fold, shuffle=True)\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for SEED in range(100, 100+n_SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    for n, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "        X_train = X.iloc[train_idx].values\n",
    "        X_test = X.iloc[test_idx].values\n",
    "        y_train = y.iloc[train_idx].values\n",
    "        y_test = y.iloc[test_idx].values\n",
    "        # normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "        params_net = {'input_size': X_train_norm.shape[1],\n",
    "                     'hidden_size': [2048,2048], # 128, 4096\n",
    "                     'output_size': y_train.shape[1],\n",
    "                     'dropout': [0.05, 0.3, 0.2]} # 长度比hidden_size长度多1\n",
    "\n",
    "        params_fit = {'X':X_train_norm,\n",
    "                     'y': y_train,\n",
    "                     'epoch': 100,\n",
    "                     'lr': 1e-4, # 1e-4 ~ 1e-3\n",
    "                     'batch_size':64, # 64, 128, 256, 512\n",
    "                     'L1': 1e-6,\n",
    "                     'L2': 1e-5,\n",
    "                     'pos_weight':1,\n",
    "                     'patience':5,\n",
    "                     'verbose':False}\n",
    "        net = DenseNet(**params_net)\n",
    "        model = Model(net)\n",
    "        model.fit(**params_fit)\n",
    "        y_train_pred = model.predict_proba(X_train_norm)\n",
    "        y_test_pred = model.predict_proba(X_test_norm)\n",
    "        train_loss.append(log_loss(y_train.ravel(), y_train_pred.ravel()))\n",
    "        test_loss.append(log_loss(y_test.ravel(), y_test_pred.ravel()))\n",
    "        print(f\"SEED {SEED-99} out of {n_SEED}, KFOLD {n+1} out of {n_fold}\")\n",
    "\n",
    "print('Training loss : ', np.average(np.array(train_loss)))\n",
    "print('Test loss : ', np.average(np.array(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[3,4,5],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 5],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
