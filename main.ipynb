{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the version of python and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install xgboost\n",
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skmultilearn.problem_transform import ClassifierChain, BinaryRelevance\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DenseNet, Model\n",
    "from evaluate import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c254b20d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DATA/lish-moa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Table of Contents  \n",
    "1. Load the dataset   \n",
    "2. Preprocessing   \n",
    "3. Models  \n",
    "    3.1 Neural Network    \n",
    "    3.2 Random Forest  \n",
    "    3.3 Classifier Chain  \n",
    "        3.3.1 Random Forest  \n",
    "        3.3.2 Logistic Regression   \n",
    "        3.3.3 Gradient Boost\n",
    "    3.4 Binary Relevance   \n",
    "        3.4.1 Random Forest  \n",
    "        3.4.2 Gradient Boost\n",
    "        3.4.3 Logistic Regression\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(data_dir+'train_features.csv', index_col='sig_id')\n",
    "y = pd.read_csv(data_dir+'train_targets_scored.csv', index_col='sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12532\n",
       "0     9367\n",
       "2     1538\n",
       "3      303\n",
       "4       55\n",
       "5       13\n",
       "7        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 875)\n",
      "(23814, 206)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Number of NA in X, and y\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# percentage of 1's in each column of y\n",
    "one_perc = y.apply(lambda x: x.sum()/len(y))\n",
    "plt.figure()\n",
    "plt.hist(one_perc, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trt_cp         21948\n",
       "ctl_vehicle     1866\n",
       "Name: cp_type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.cp_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D1    12147\n",
       "D2    11667\n",
       "Name: cp_dose, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.cp_dose.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for cp_type and cp_dose\n",
    "X['cp_type'].replace({'trt_cp':1., 'ctl_vehicle':0.}, inplace=True)\n",
    "X['cp_dose'].replace({'D1':1., 'D2':0.}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training set and test set\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "\n",
    "ids = X.index.values.copy()\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "train_perc, val_perc, test_perc = 0.7, 0.1, 0.2\n",
    "train_id = ids[:round(len(ids)*train_perc)]\n",
    "val_id = ids[round(len(ids)*train_perc):round(len(ids)*(train_perc+val_perc))]\n",
    "test_id = ids[round(len(ids)*(train_perc+val_perc)):]\n",
    "\n",
    "X_train = X.loc[train_id]\n",
    "X_val = X.loc[val_id]\n",
    "X_test = X.loc[test_id]\n",
    "\n",
    "y_train = y.loc[train_id]\n",
    "y_val = y.loc[val_id]\n",
    "y_test = y.loc[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_train_norm.columns = X_train.columns\n",
    "X_train_norm.index = X_train.index\n",
    "\n",
    "X_val_norm = pd.DataFrame(scaler.transform(X_val))\n",
    "X_val_norm.columns = X_val.columns\n",
    "X_val_norm.index = X_val.index\n",
    "\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_norm.columns = X_test.columns\n",
    "X_test_norm.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training set: 0.00072\n",
      "Test set: 0.00063\n",
      "\n",
      "1\n",
      "Training set: 0.00072\n",
      "Test set: 0.00126\n",
      "\n",
      "2\n",
      "Training set: 0.00102\n",
      "Test set: 0.00105\n",
      "\n",
      "3\n",
      "Training set: 0.007678\n",
      "Test set: 0.008398\n",
      "\n",
      "4\n",
      "Training set: 0.012657\n",
      "Test set: 0.013017\n",
      "\n",
      "5\n",
      "Training set: 0.002879\n",
      "Test set: 0.004409\n",
      "\n",
      "6\n",
      "Training set: 0.002519\n",
      "Test set: 0.00168\n",
      "\n",
      "7\n",
      "Training set: 0.003719\n",
      "Test set: 0.005039\n",
      "\n",
      "8\n",
      "Training set: 0.00048\n",
      "Test set: 0.00063\n",
      "\n",
      "9\n",
      "Training set: 0.010918\n",
      "Test set: 0.013017\n",
      "\n",
      "10\n",
      "Training set: 0.015717\n",
      "Test set: 0.013857\n",
      "\n",
      "11\n",
      "Training set: 0.002759\n",
      "Test set: 0.002519\n",
      "\n",
      "12\n",
      "Training set: 0.00024\n",
      "Test set: 0.00021\n",
      "\n",
      "13\n",
      "Training set: 0.00168\n",
      "Test set: 0.00084\n",
      "\n",
      "14\n",
      "Training set: 0.00042\n",
      "Test set: 0.00042\n",
      "\n",
      "15\n",
      "Training set: 0.00042\n",
      "Test set: 0.00063\n",
      "\n",
      "16\n",
      "Training set: 0.00228\n",
      "Test set: 0.00126\n",
      "\n",
      "17\n",
      "Training set: 0.003419\n",
      "Test set: 0.003569\n",
      "\n",
      "18\n",
      "Training set: 0.003659\n",
      "Test set: 0.002729\n",
      "\n",
      "19\n",
      "Training set: 0.0018\n",
      "Test set: 0.00105\n",
      "\n",
      "20\n",
      "Training set: 0.00132\n",
      "Test set: 0.0021\n",
      "\n",
      "21\n",
      "Training set: 0.002939\n",
      "Test set: 0.002939\n",
      "\n",
      "22\n",
      "Training set: 0.00012\n",
      "Test set: 0.00084\n",
      "\n",
      "23\n",
      "Training set: 0.00192\n",
      "Test set: 0.00168\n",
      "\n",
      "24\n",
      "Training set: 0.00036\n",
      "Test set: 0.00063\n",
      "\n",
      "25\n",
      "Training set: 0.0006\n",
      "Test set: 0.00042\n",
      "\n",
      "26\n",
      "Training set: 0.00042\n",
      "Test set: 0.00084\n",
      "\n",
      "27\n",
      "Training set: 0.00078\n",
      "Test set: 0.00063\n",
      "\n",
      "28\n",
      "Training set: 0.003179\n",
      "Test set: 0.002729\n",
      "\n",
      "29\n",
      "Training set: 0.00144\n",
      "Test set: 0.00168\n",
      "\n",
      "30\n",
      "Training set: 0.00096\n",
      "Test set: 0.00084\n",
      "\n",
      "31\n",
      "Training set: 0.00198\n",
      "Test set: 0.00168\n",
      "\n",
      "32\n",
      "Training set: 0.00198\n",
      "Test set: 0.002309\n",
      "\n",
      "33\n",
      "Training set: 0.0003\n",
      "Test set: 0.00021\n",
      "\n",
      "34\n",
      "Training set: 6e-05\n",
      "Test set: 0.0\n",
      "\n",
      "35\n",
      "Training set: 0.00036\n",
      "Test set: 0.00105\n",
      "\n",
      "36\n",
      "Training set: 0.003719\n",
      "Test set: 0.005879\n",
      "\n",
      "37\n",
      "Training set: 0.00072\n",
      "Test set: 0.00105\n",
      "\n",
      "38\n",
      "Training set: 0.004559\n",
      "Test set: 0.003149\n",
      "\n",
      "39\n",
      "Training set: 0.0003\n",
      "Test set: 0.00021\n",
      "\n",
      "40\n",
      "Training set: 0.002519\n",
      "Test set: 0.0021\n",
      "\n",
      "41\n",
      "Training set: 0.003119\n",
      "Test set: 0.004619\n",
      "\n",
      "42\n",
      "Training set: 0.0015\n",
      "Test set: 0.00126\n",
      "\n",
      "43\n",
      "Training set: 0.007978\n",
      "Test set: 0.009028\n",
      "\n",
      "44\n",
      "Training set: 0.003839\n",
      "Test set: 0.002939\n",
      "\n",
      "45\n",
      "Training set: 0.005519\n",
      "Test set: 0.002939\n",
      "\n",
      "46\n",
      "Training set: 0.00024\n",
      "Test set: 0.00042\n",
      "\n",
      "47\n",
      "Training set: 0.0012\n",
      "Test set: 0.00084\n",
      "\n",
      "48\n",
      "Training set: 0.00162\n",
      "Test set: 0.00168\n",
      "\n",
      "49\n",
      "Training set: 0.003059\n",
      "Test set: 0.00126\n",
      "\n",
      "50\n",
      "Training set: 0.00084\n",
      "Test set: 0.00147\n",
      "\n",
      "51\n",
      "Training set: 0.002999\n",
      "Test set: 0.002939\n",
      "\n",
      "52\n",
      "Training set: 0.00114\n",
      "Test set: 0.00147\n",
      "\n",
      "53\n",
      "Training set: 0.00036\n",
      "Test set: 0.0\n",
      "\n",
      "54\n",
      "Training set: 0.011938\n",
      "Test set: 0.011127\n",
      "\n",
      "55\n",
      "Training set: 0.0021\n",
      "Test set: 0.00084\n",
      "\n",
      "56\n",
      "Training set: 0.00222\n",
      "Test set: 0.002729\n",
      "\n",
      "57\n",
      "Training set: 0.0015\n",
      "Test set: 0.00147\n",
      "\n",
      "58\n",
      "Training set: 0.0015\n",
      "Test set: 0.00168\n",
      "\n",
      "59\n",
      "Training set: 0.00066\n",
      "Test set: 0.00084\n",
      "\n",
      "60\n",
      "Training set: 0.00054\n",
      "Test set: 0.00042\n",
      "\n",
      "61\n",
      "Training set: 0.004379\n",
      "Test set: 0.002939\n",
      "\n",
      "62\n",
      "Training set: 0.0009\n",
      "Test set: 0.00021\n",
      "\n",
      "63\n",
      "Training set: 0.013617\n",
      "Test set: 0.014907\n",
      "\n",
      "64\n",
      "Training set: 0.00228\n",
      "Test set: 0.00189\n",
      "\n",
      "65\n",
      "Training set: 0.00102\n",
      "Test set: 0.00105\n",
      "\n",
      "66\n",
      "Training set: 0.00204\n",
      "Test set: 0.00105\n",
      "\n",
      "67\n",
      "Training set: 0.00216\n",
      "Test set: 0.00168\n",
      "\n",
      "68\n",
      "Training set: 0.00246\n",
      "Test set: 0.0021\n",
      "\n",
      "69\n",
      "Training set: 0.00024\n",
      "Test set: 0.00042\n",
      "\n",
      "70\n",
      "Training set: 0.00186\n",
      "Test set: 0.00105\n",
      "\n",
      "71\n",
      "Training set: 0.017516\n",
      "Test set: 0.021415\n",
      "\n",
      "72\n",
      "Training set: 0.003659\n",
      "Test set: 0.004829\n",
      "\n",
      "73\n",
      "Training set: 0.00144\n",
      "Test set: 0.00147\n",
      "\n",
      "74\n",
      "Training set: 0.00114\n",
      "Test set: 0.00105\n",
      "\n",
      "75\n",
      "Training set: 0.0003\n",
      "Test set: 0.0\n",
      "\n",
      "76\n",
      "Training set: 0.00198\n",
      "Test set: 0.00189\n",
      "\n",
      "77\n",
      "Training set: 0.017397\n",
      "Test set: 0.015117\n",
      "\n",
      "78\n",
      "Training set: 0.004979\n",
      "Test set: 0.004409\n",
      "\n",
      "79\n",
      "Training set: 0.018476\n",
      "Test set: 0.016376\n",
      "\n",
      "80\n",
      "Training set: 0.014577\n",
      "Test set: 0.012597\n",
      "\n",
      "81\n",
      "Training set: 0.00018\n",
      "Test set: 0.00063\n",
      "\n",
      "82\n",
      "Training set: 6e-05\n",
      "Test set: 0.0\n",
      "\n",
      "83\n",
      "Training set: 0.007019\n",
      "Test set: 0.004829\n",
      "\n",
      "84\n",
      "Training set: 0.0018\n",
      "Test set: 0.003359\n",
      "\n",
      "85\n",
      "Training set: 0.00132\n",
      "Test set: 0.00105\n",
      "\n",
      "86\n",
      "Training set: 0.00078\n",
      "Test set: 0.00021\n",
      "\n",
      "87\n",
      "Training set: 0.00114\n",
      "Test set: 0.00084\n",
      "\n",
      "88\n",
      "Training set: 0.00198\n",
      "Test set: 0.002309\n",
      "\n",
      "89\n",
      "Training set: 0.011398\n",
      "Test set: 0.012177\n",
      "\n",
      "90\n",
      "Training set: 0.00072\n",
      "Test set: 0.00084\n",
      "\n",
      "91\n",
      "Training set: 0.00078\n",
      "Test set: 0.00063\n",
      "\n",
      "92\n",
      "Training set: 0.00084\n",
      "Test set: 0.00126\n",
      "\n",
      "93\n",
      "Training set: 0.004199\n",
      "Test set: 0.004619\n",
      "\n",
      "94\n",
      "Training set: 0.006059\n",
      "Test set: 0.009238\n",
      "\n",
      "95\n",
      "Training set: 0.00222\n",
      "Test set: 0.002729\n",
      "\n",
      "96\n",
      "Training set: 0.011698\n",
      "Test set: 0.010917\n",
      "\n",
      "97\n",
      "Training set: 0.0006\n",
      "Test set: 0.00042\n",
      "\n",
      "98\n",
      "Training set: 0.003059\n",
      "Test set: 0.003779\n",
      "\n",
      "99\n",
      "Training set: 0.015117\n",
      "Test set: 0.018266\n",
      "\n",
      "100\n",
      "Training set: 0.00084\n",
      "Test set: 0.00042\n",
      "\n",
      "101\n",
      "Training set: 0.00216\n",
      "Test set: 0.003149\n",
      "\n",
      "102\n",
      "Training set: 0.003299\n",
      "Test set: 0.002729\n",
      "\n",
      "103\n",
      "Training set: 0.004619\n",
      "Test set: 0.004199\n",
      "\n",
      "104\n",
      "Training set: 0.00246\n",
      "Test set: 0.002729\n",
      "\n",
      "105\n",
      "Training set: 0.010618\n",
      "Test set: 0.008398\n",
      "\n",
      "106\n",
      "Training set: 0.00102\n",
      "Test set: 0.00084\n",
      "\n",
      "107\n",
      "Training set: 0.0015\n",
      "Test set: 0.00084\n",
      "\n",
      "108\n",
      "Training set: 0.003179\n",
      "Test set: 0.002939\n",
      "\n",
      "109\n",
      "Training set: 0.011038\n",
      "Test set: 0.013017\n",
      "\n",
      "110\n",
      "Training set: 0.003839\n",
      "Test set: 0.004199\n",
      "\n",
      "111\n",
      "Training set: 0.00138\n",
      "Test set: 0.00189\n",
      "\n",
      "112\n",
      "Training set: 0.00138\n",
      "Test set: 0.00063\n",
      "\n",
      "113\n",
      "Training set: 0.00156\n",
      "Test set: 0.00021\n",
      "\n",
      "114\n",
      "Training set: 0.003239\n",
      "Test set: 0.002939\n",
      "\n",
      "115\n",
      "Training set: 0.00108\n",
      "Test set: 0.00189\n",
      "\n",
      "116\n",
      "Training set: 0.00192\n",
      "Test set: 0.003359\n",
      "\n",
      "117\n",
      "Training set: 0.00192\n",
      "Test set: 0.00147\n",
      "\n",
      "118\n",
      "Training set: 0.003899\n",
      "Test set: 0.003149\n",
      "\n",
      "119\n",
      "Training set: 0.011158\n",
      "Test set: 0.011757\n",
      "\n",
      "120\n",
      "Training set: 0.00018\n",
      "Test set: 0.00021\n",
      "\n",
      "121\n",
      "Training set: 0.00012\n",
      "Test set: 0.00084\n",
      "\n",
      "122\n",
      "Training set: 0.00246\n",
      "Test set: 0.003149\n",
      "\n",
      "123\n",
      "Training set: 0.0003\n",
      "Test set: 0.00147\n",
      "\n",
      "124\n",
      "Training set: 0.002699\n",
      "Test set: 0.002939\n",
      "\n",
      "125\n",
      "Training set: 0.00024\n",
      "Test set: 0.00042\n",
      "\n",
      "126\n",
      "Training set: 0.00132\n",
      "Test set: 0.00147\n",
      "\n",
      "127\n",
      "Training set: 0.003179\n",
      "Test set: 0.003149\n",
      "\n",
      "128\n",
      "Training set: 0.002639\n",
      "Test set: 0.005669\n",
      "\n",
      "129\n",
      "Training set: 0.00102\n",
      "Test set: 0.00105\n",
      "\n",
      "130\n",
      "Training set: 0.00042\n",
      "Test set: 0.00021\n",
      "\n",
      "131\n",
      "Training set: 0.003839\n",
      "Test set: 0.002729\n",
      "\n",
      "132\n",
      "Training set: 0.00084\n",
      "Test set: 0.00042\n",
      "\n",
      "133\n",
      "Training set: 0.005399\n",
      "Test set: 0.006299\n",
      "\n",
      "134\n",
      "Training set: 0.00216\n",
      "Test set: 0.00147\n",
      "\n",
      "135\n",
      "Training set: 0.00198\n",
      "Test set: 0.00042\n",
      "\n",
      "136\n",
      "Training set: 0.033413\n",
      "Test set: 0.040941\n",
      "\n",
      "137\n",
      "Training set: 0.00018\n",
      "Test set: 0.00063\n",
      "\n",
      "138\n",
      "Training set: 0.00114\n",
      "Test set: 0.00084\n",
      "\n",
      "139\n",
      "Training set: 0.0006\n",
      "Test set: 0.00021\n",
      "\n",
      "140\n",
      "Training set: 0.0012\n",
      "Test set: 0.00084\n",
      "\n",
      "141\n",
      "Training set: 0.00024\n",
      "Test set: 0.00063\n",
      "\n",
      "142\n",
      "Training set: 0.00078\n",
      "Test set: 0.00063\n",
      "\n",
      "143\n",
      "Training set: 0.002639\n",
      "Test set: 0.002309\n",
      "\n",
      "144\n",
      "Training set: 0.004499\n",
      "Test set: 0.002729\n",
      "\n",
      "145\n",
      "Training set: 0.00156\n",
      "Test set: 0.00189\n",
      "\n",
      "146\n",
      "Training set: 0.0024\n",
      "Test set: 0.002729\n",
      "\n",
      "147\n",
      "Training set: 0.0009\n",
      "Test set: 0.00168\n",
      "\n",
      "148\n",
      "Training set: 0.002579\n",
      "Test set: 0.00189\n",
      "\n",
      "149\n",
      "Training set: 0.012358\n",
      "Test set: 0.011967\n",
      "\n",
      "150\n",
      "Training set: 0.00072\n",
      "Test set: 0.00084\n",
      "\n",
      "151\n",
      "Training set: 0.011218\n",
      "Test set: 0.009238\n",
      "\n",
      "152\n",
      "Training set: 0.00102\n",
      "Test set: 0.00147\n",
      "\n",
      "153\n",
      "Training set: 0.006959\n",
      "Test set: 0.005249\n",
      "\n",
      "154\n",
      "Training set: 0.0009\n",
      "Test set: 0.002729\n",
      "\n",
      "155\n",
      "Training set: 0.00204\n",
      "Test set: 0.003779\n",
      "\n",
      "156\n",
      "Training set: 0.004559\n",
      "Test set: 0.002729\n",
      "\n",
      "157\n",
      "Training set: 0.004499\n",
      "Test set: 0.005249\n",
      "\n",
      "158\n",
      "Training set: 0.0015\n",
      "Test set: 0.00063\n",
      "\n",
      "159\n",
      "Training set: 0.004859\n",
      "Test set: 0.006089\n",
      "\n",
      "160\n",
      "Training set: 0.00084\n",
      "Test set: 0.00063\n",
      "\n",
      "161\n",
      "Training set: 0.00168\n",
      "Test set: 0.00084\n",
      "\n",
      "162\n",
      "Training set: 0.003719\n",
      "Test set: 0.003569\n",
      "\n",
      "163\n",
      "Training set: 0.029034\n",
      "Test set: 0.035062\n",
      "\n",
      "164\n",
      "Training set: 0.0018\n",
      "Test set: 0.002939\n",
      "\n",
      "165\n",
      "Training set: 0.00024\n",
      "Test set: 0.00021\n",
      "\n",
      "166\n",
      "Training set: 0.004559\n",
      "Test set: 0.003359\n",
      "\n",
      "167\n",
      "Training set: 0.0006\n",
      "Test set: 0.00168\n",
      "\n",
      "168\n",
      "Training set: 0.00222\n",
      "Test set: 0.002519\n",
      "\n",
      "169\n",
      "Training set: 0.009838\n",
      "Test set: 0.007768\n",
      "\n",
      "170\n",
      "Training set: 0.00042\n",
      "Test set: 0.00084\n",
      "\n",
      "171\n",
      "Training set: 0.002819\n",
      "Test set: 0.002939\n",
      "\n",
      "172\n",
      "Training set: 0.00012\n",
      "Test set: 0.00084\n",
      "\n",
      "173\n",
      "Training set: 0.00144\n",
      "Test set: 0.0021\n",
      "\n",
      "174\n",
      "Training set: 0.0015\n",
      "Test set: 0.00168\n",
      "\n",
      "175\n",
      "Training set: 0.00096\n",
      "Test set: 0.00126\n",
      "\n",
      "176\n",
      "Training set: 0.010078\n",
      "Test set: 0.010498\n",
      "\n",
      "177\n",
      "Training set: 0.017696\n",
      "Test set: 0.015956\n",
      "\n",
      "178\n",
      "Training set: 0.00198\n",
      "Test set: 0.00189\n",
      "\n",
      "179\n",
      "Training set: 0.00132\n",
      "Test set: 0.00189\n",
      "\n",
      "180\n",
      "Training set: 0.00138\n",
      "Test set: 0.00189\n",
      "\n",
      "181\n",
      "Training set: 0.00108\n",
      "Test set: 0.00126\n",
      "\n",
      "182\n",
      "Training set: 0.010618\n",
      "Test set: 0.013437\n",
      "\n",
      "183\n",
      "Training set: 0.00108\n",
      "Test set: 0.00147\n",
      "\n",
      "184\n",
      "Training set: 0.003479\n",
      "Test set: 0.00063\n",
      "\n",
      "185\n",
      "Training set: 0.0003\n",
      "Test set: 0.00021\n",
      "\n",
      "186\n",
      "Training set: 0.00072\n",
      "Test set: 0.00084\n",
      "\n",
      "187\n",
      "Training set: 0.003119\n",
      "Test set: 0.00105\n",
      "\n",
      "188\n",
      "Training set: 0.00096\n",
      "Test set: 0.00189\n",
      "\n",
      "189\n",
      "Training set: 0.00084\n",
      "Test set: 0.00063\n",
      "\n",
      "190\n",
      "Training set: 0.0015\n",
      "Test set: 0.00168\n",
      "\n",
      "191\n",
      "Training set: 0.0015\n",
      "Test set: 0.00042\n",
      "\n",
      "192\n",
      "Training set: 0.0003\n",
      "Test set: 0.00042\n",
      "\n",
      "193\n",
      "Training set: 0.0018\n",
      "Test set: 0.00042\n",
      "\n",
      "194\n",
      "Training set: 0.005159\n",
      "Test set: 0.005459\n",
      "\n",
      "195\n",
      "Training set: 0.00084\n",
      "Test set: 0.00021\n",
      "\n",
      "196\n",
      "Training set: 0.00024\n",
      "Test set: 0.0\n",
      "\n",
      "197\n",
      "Training set: 0.00096\n",
      "Test set: 0.00084\n",
      "\n",
      "198\n",
      "Training set: 0.00222\n",
      "Test set: 0.00147\n",
      "\n",
      "199\n",
      "Training set: 0.013197\n",
      "Test set: 0.011127\n",
      "\n",
      "200\n",
      "Training set: 0.003059\n",
      "Test set: 0.003149\n",
      "\n",
      "201\n",
      "Training set: 0.00024\n",
      "Test set: 0.00042\n",
      "\n",
      "202\n",
      "Training set: 0.006959\n",
      "Test set: 0.007768\n",
      "\n",
      "203\n",
      "Training set: 0.0009\n",
      "Test set: 0.00168\n",
      "\n",
      "204\n",
      "Training set: 0.00174\n",
      "Test set: 0.00189\n",
      "\n",
      "205\n",
      "Training set: 0.0012\n",
      "Test set: 0.00063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the class skewness in the training and testing set\n",
    "for icol in range(y_train.shape[1]):\n",
    "    print(icol)\n",
    "    train_icol = y_train.iloc[:,icol]\n",
    "    print('Training set: {}'.format(round(sum(train_icol)/len(train_icol), 6)) )\n",
    "    test_icol = y_test.iloc[:,icol]\n",
    "    print('Test set: {}'.format(round(sum(test_icol)/len(test_icol), 6)) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models  \n",
    "\n",
    "### Model 1: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1, 0] : loss 1.047478199005127\n",
      "Epoch [1, 3840] : loss 1.05042564868927\n",
      "Epoch [1, 7680] : loss 1.0482022762298584\n",
      "Epoch [1, 11520] : loss 1.057258129119873\n",
      "Epoch [1, 15360] : loss 1.056621789932251\n",
      "Epoch [2, 0] : loss 1.0446748733520508\n",
      "Epoch [2, 3840] : loss 1.0472965240478516\n",
      "Epoch [2, 7680] : loss 1.0443134307861328\n",
      "Epoch [2, 11520] : loss 1.0522059202194214\n",
      "Epoch [2, 15360] : loss 1.049384593963623\n",
      "Epoch [3, 0] : loss 1.031517505645752\n",
      "Epoch [3, 3840] : loss 1.030834436416626\n",
      "Epoch [3, 7680] : loss 1.0171416997909546\n",
      "Epoch [3, 11520] : loss 1.0096237659454346\n",
      "Epoch [3, 15360] : loss 0.9825803637504578\n",
      "Epoch [4, 0] : loss 0.9295281767845154\n",
      "Epoch [4, 3840] : loss 0.8986634016036987\n",
      "Epoch [4, 7680] : loss 0.858842134475708\n",
      "Epoch [4, 11520] : loss 0.8441187739372253\n",
      "Epoch [4, 15360] : loss 0.8327736258506775\n",
      "Epoch [5, 0] : loss 0.8097099661827087\n",
      "Epoch [5, 3840] : loss 0.8154553174972534\n",
      "Epoch [5, 7680] : loss 0.8103313446044922\n",
      "Epoch [5, 11520] : loss 0.8212631940841675\n",
      "Epoch [5, 15360] : loss 0.8201394081115723\n",
      "Epoch [6, 0] : loss 0.8013418912887573\n",
      "Epoch [6, 3840] : loss 0.8102124929428101\n",
      "Epoch [6, 7680] : loss 0.8063452243804932\n",
      "Epoch [6, 11520] : loss 0.8192839622497559\n",
      "Epoch [6, 15360] : loss 0.8184539079666138\n",
      "Epoch [7, 0] : loss 0.7999599575996399\n",
      "Epoch [7, 3840] : loss 0.8091217875480652\n",
      "Epoch [7, 7680] : loss 0.8053784966468811\n",
      "Epoch [7, 11520] : loss 0.8187475204467773\n",
      "Epoch [7, 15360] : loss 0.8179959058761597\n",
      "Epoch [8, 0] : loss 0.7992702126502991\n",
      "Epoch [8, 3840] : loss 0.8086432218551636\n",
      "Epoch [8, 7680] : loss 0.8047776222229004\n",
      "Epoch [8, 11520] : loss 0.8185872435569763\n",
      "Epoch [8, 15360] : loss 0.8176785111427307\n",
      "Epoch [9, 0] : loss 0.7989540696144104\n",
      "Epoch [9, 3840] : loss 0.8083202242851257\n",
      "Epoch [9, 7680] : loss 0.8044001460075378\n",
      "Epoch [9, 11520] : loss 0.8180893063545227\n",
      "Epoch [9, 15360] : loss 0.8173395395278931\n",
      "Epoch [10, 0] : loss 0.7985403537750244\n",
      "Epoch [10, 3840] : loss 0.8079520463943481\n",
      "Epoch [10, 7680] : loss 0.8040676712989807\n",
      "Epoch [10, 11520] : loss 0.8177434206008911\n",
      "Epoch [10, 15360] : loss 0.8170088529586792\n"
     ]
    }
   ],
   "source": [
    "params_net = {'input_size': X_train_norm.shape[1],\n",
    "             'hidden_size': [512, 512, 256, 128 ,256, 512, 1024],\n",
    "             'output_size': y_train.shape[1],\n",
    "             'dropout': 0.01}\n",
    "\n",
    "params_fit = {'X':X_train_norm,\n",
    "             'y': y_train,\n",
    "             'epoch': 10,\n",
    "             'lr': 5e-6,\n",
    "             'batch_size':128,\n",
    "             'L2': 0,\n",
    "             'pos_weight':50,\n",
    "             'verbose':True}\n",
    "\n",
    "net = DenseNet(**params_net)\n",
    "model = Model(net)\n",
    "model.fit(**params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: \n",
      "0.05716523101018011\n",
      "Recall: \n",
      "0.08687244328746746\n",
      "F1: \n",
      "0.06895533843020338\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = model.predict(X_train_norm, 0.45)\n",
    "# print('Accuracy: ')\n",
    "# print(accuracy_score(y_train, y_train_predict))\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "y_test_predict = model.predict(X_test_norm, 0.5)\n",
    "# print('Accuracy: ')\n",
    "# print(accuracy_score(y_test, y_test_predict))\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "20432\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: grid search for pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: define class-weight based \n",
    "weight_ratio = 50\n",
    "class_weight = [{0:1, 1:round(len(value)/sum(value)-1)* weight_ratio} for _, value in y_train.iteritems()]\n",
    "\n",
    "# class_weight = [{0:1, 1:50}]*y_train.shape[1]\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 100,\n",
    "                             max_depth = 20,\n",
    "                             class_weight = 'balanced_subsample',\n",
    "                             random_state=100)\n",
    "rfc.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: \n",
      "1.0\n",
      "Recall: \n",
      "0.2786165860914838\n",
      "F1: \n",
      "0.4358094351695655\n",
      "\n",
      "Precision: \n",
      "0.9689265536723164\n",
      "Recall: \n",
      "0.1009417304296645\n",
      "F1: \n",
      "0.1828358208955224\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = rfc.predict(X_train)\n",
    "\n",
    "# print('Accuracy: ')\n",
    "# print(accuracy_score(y_train, y_train_predict))\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "\n",
    "print()\n",
    "\n",
    "y_test_predict = rfc.predict(X_test)\n",
    "\n",
    "# print('Accuracy: ')\n",
    "# print(accuracy_score(y_test, y_test_predict))\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "3746\n",
      "\n",
      "3398\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 : Classifier Chains   \n",
    "#### Model 3.1 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg_vs_pos = [round(len(y)/sum(y)-1) for _, y in y_train.iteritems()];\n",
    "plt.figure()\n",
    "plt.hist(neg_vs_pos, bins=100, range = (0, 2000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(neg_vs_pos)*0.76>8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [13:05,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "weight_ratio = 0.76\n",
    "max_depth = 3\n",
    "max_features = 20\n",
    "n_estimators = 120\n",
    "min_samples_leaf = 1\n",
    "\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_chain = []\n",
    "\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X = X_train_val.values\n",
    "X2 = X_test.values\n",
    "for _, y in tqdm(y_train_val.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    class_weight = {0:1, 1:min(round(len(y)/sum(y)-1)*weight_ratio, 8000)}\n",
    "    \n",
    "    rfc_2 = RandomForestClassifier(n_estimators = n_estimators,\n",
    "                                  max_depth = max_depth,\n",
    "                                  max_features = max_features,\n",
    "                                  min_samples_leaf  = min_samples_leaf,\n",
    "                                  class_weight = class_weight,\n",
    "                                  n_jobs = 3,\n",
    "                                  random_state=100);\n",
    "    rfc_2.fit(X, y);\n",
    "    \n",
    "    y_predict = rfc_2.predict(X).reshape((-1, 1))\n",
    "    y_predict2 = rfc_2.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    \n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "        \n",
    "    X = np.concatenate((X, y_predict), axis=1)\n",
    "    X2 = np.concatenate((X2, y_predict2), axis=1)\n",
    "    model_chain.append(rfc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inference\n",
    "# y_train_predict = np.array([])\n",
    "# y_test_predict = np.array([])\n",
    "\n",
    "# X_train_val = pd.concat([X_train, X_val])\n",
    "# X = X_train_val.values\n",
    "# X2 = X_test.values\n",
    "# for rfc_2 in model_chain:\n",
    "#     y_predict = rfc_2.predict(X).reshape((-1, 1))\n",
    "#     y_predict2 = rfc_2.predict(X2).reshape((-1,1))\n",
    "#     if y_test_predict.size == 0:\n",
    "#         y_test_predict = y_predict2\n",
    "#     else:\n",
    "#         y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "#     if y_train_predict.size == 0:\n",
    "#         y_train_predict = y_predict\n",
    "#     else:\n",
    "#         y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "#     X = np.concatenate((X, y_predict), axis=1)\n",
    "#     X2 = np.concatenate((X2, y_predict2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weight_ratio: ', weight_ratio, ' max_depth: ', max_depth, ' max_features:',max_features, ' n_estimators:',n_estimators, ' min_samples_leaf:',min_samples_leaf)\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "13838\n",
      "\n",
      "3398\n",
      "2563\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3.2 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [1:24:49, 24.71s/it]\n"
     ]
    }
   ],
   "source": [
    "weight_ratio = 0.5\n",
    "penalty = 'l1'\n",
    "C = 0.5\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_chain_lr = []\n",
    "\n",
    "X_train_val_norm = pd.concat([X_train_norm, X_val_norm])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X = X_train_val_norm.values\n",
    "X2 = X_test_norm.values\n",
    "for _, y in tqdm(y_train_val.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    class_weight = {0:1, 1:min(round(len(y)/sum(y)-1)*weight_ratio, 8000)}\n",
    "    \n",
    "    lr_1 = LogisticRegression(penalty = penalty,\n",
    "                            C = C,\n",
    "                            class_weight = class_weight,\n",
    "                            n_jobs = -1,\n",
    "                            solver = 'saga',\n",
    "                            random_state = 100)\n",
    "    lr_1.fit(X, y);\n",
    "    \n",
    "    y_predict = lr_1.predict(X).reshape((-1, 1))\n",
    "    y_predict2 = lr_1.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    \n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    X = np.concatenate((X, scaler.fit_transform(y_predict)), axis=1)\n",
    "    X2 = np.concatenate((X2, scaler.transform(y_predict2)), axis=1)\n",
    "    model_chain_lr.append(lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty: l1  C: 0.5  posi_weight: 25\n",
      "Precision: \n",
      "0.28820592621409585\n",
      "Recall: \n",
      "0.9918185198958721\n",
      "F1: \n",
      "0.44662893123890546\n",
      "\n",
      "Precision: \n",
      "0.10052567731500202\n",
      "Recall: \n",
      "0.3658034137728075\n",
      "F1: \n",
      "0.15771109560362875\n"
     ]
    }
   ],
   "source": [
    "print('weight_ratio:',weight_ratio, 'penalty:',penalty, ' C:',C, ')\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "46269\n",
      "\n",
      "3398\n",
      "12365\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3.3 : Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "1it [00:38, 38.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "2it [01:15, 38.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "3it [01:57, 39.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "4it [03:15, 50.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "5it [04:46, 62.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "6it [05:39, 60.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "7it [06:42, 60.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "8it [07:42, 60.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "9it [08:23, 54.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "10it [09:47, 63.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "11it [11:41, 78.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "12it [12:27, 68.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "13it [12:56, 56.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "14it [13:54, 57.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "15it [14:36, 52.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "16it [15:08, 46.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "17it [16:10, 51.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "18it [17:17, 55.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "19it [18:21, 58.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "20it [19:19, 58.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "21it [20:07, 55.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "22it [21:07, 56.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "23it [21:32, 47.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "24it [22:26, 49.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "25it [23:02, 45.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "26it [23:43, 44.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "27it [24:16, 40.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "28it [25:00, 41.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "29it [26:05, 48.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "30it [27:08, 52.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "31it [27:57, 51.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "32it [29:03, 55.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "33it [29:55, 54.75s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "34it [30:28, 48.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "35it [30:57, 42.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "36it [31:24, 37.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "37it [32:20, 43.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "38it [33:01, 42.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "39it [34:11, 50.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "40it [34:55, 48.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "41it [35:55, 52.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "42it [36:54, 54.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "43it [37:48, 54.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "44it [39:05, 60.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "45it [40:18, 64.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "46it [41:40, 69.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "47it [42:10, 57.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "48it [42:53, 53.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "49it [43:42, 52.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "50it [45:08, 62.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "51it [45:52, 56.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "52it [46:35, 52.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "53it [47:19, 50.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "54it [47:58, 46.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "55it [49:41, 63.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "56it [50:42, 62.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "57it [51:39, 61.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "58it [52:32, 58.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "59it [53:16, 54.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "60it [53:47, 47.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "61it [54:28, 45.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "62it [55:42, 53.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "63it [56:42, 55.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "64it [57:37, 55.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "65it [58:32, 55.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "66it [59:07, 49.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "67it [1:00:09, 53.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "68it [1:01:09, 55.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "69it [1:02:13, 57.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "70it [1:02:39, 48.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "71it [1:03:29, 48.81s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "72it [1:05:16, 66.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "73it [1:06:34, 69.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "74it [1:07:19, 62.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "75it [1:08:06, 57.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "76it [1:08:54, 54.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "77it [1:09:50, 55.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "78it [1:11:47, 73.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "79it [1:13:08, 75.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "80it [1:15:01, 87.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "81it [1:16:08, 81.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "82it [1:16:32, 63.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "83it [1:16:59, 53.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "84it [1:18:17, 60.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "85it [1:18:59, 54.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "86it [1:19:49, 53.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "87it [1:20:30, 49.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "88it [1:21:24, 50.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "89it [1:22:03, 47.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "90it [1:23:06, 52.23s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "91it [1:25:13, 74.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "92it [1:26:02, 67.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "93it [1:26:44, 59.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "94it [1:27:52, 61.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "95it [1:29:00, 63.84s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "96it [1:29:43, 57.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "97it [1:30:21, 51.66s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "98it [1:30:58, 47.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "99it [1:31:58, 51.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100it [1:33:41, 66.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "101it [1:34:27, 60.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "102it [1:35:10, 55.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "103it [1:36:12, 57.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "104it [1:36:59, 54.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "105it [1:37:59, 55.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "106it [1:39:40, 69.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "107it [1:40:28, 63.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "108it [1:41:18, 59.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "109it [1:42:18, 59.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "110it [1:43:26, 61.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "111it [1:44:27, 61.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "112it [1:45:16, 57.84s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "113it [1:46:05, 55.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "114it [1:47:25, 62.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "115it [1:48:25, 61.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "116it [1:49:10, 56.75s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "117it [1:50:02, 55.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "118it [1:50:59, 55.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "119it [1:52:04, 58.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "120it [1:52:48, 54.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "121it [1:53:19, 47.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "122it [1:53:41, 39.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "123it [1:54:37, 44.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "124it [1:55:01, 38.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "125it [1:55:52, 42.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "126it [1:56:17, 36.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "127it [1:56:55, 37.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "128it [1:57:39, 39.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "129it [1:58:26, 41.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "130it [1:59:08, 41.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "131it [1:59:50, 41.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "132it [2:15:25, 309.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "133it [2:16:13, 231.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "134it [2:17:21, 182.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "135it [2:18:29, 148.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "136it [2:19:46, 126.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "137it [2:20:54, 108.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "138it [2:21:22, 84.81s/it] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "139it [2:22:26, 78.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "140it [2:23:24, 72.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "141it [2:24:26, 69.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "142it [2:24:53, 56.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "143it [2:25:35, 52.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "144it [2:26:53, 60.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "145it [2:28:27, 70.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "146it [2:29:16, 63.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "147it [2:30:02, 58.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "148it [2:30:36, 51.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "149it [2:31:45, 56.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "150it [2:32:23, 51.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "151it [2:33:09, 49.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "152it [2:35:04, 69.23s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "153it [2:35:54, 63.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "154it [2:37:08, 66.68s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "155it [2:37:52, 59.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "156it [2:38:43, 57.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "157it [2:40:03, 64.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "158it [2:41:22, 68.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "159it [2:42:29, 68.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "160it [2:43:35, 67.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "161it [2:44:16, 59.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "162it [2:45:16, 59.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "163it [2:46:21, 61.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "164it [2:46:52, 52.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "165it [2:47:42, 51.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "166it [2:48:14, 45.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "167it [2:49:23, 52.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "168it [2:49:54, 46.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "169it [2:50:52, 49.71s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "170it [2:51:55, 53.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "171it [2:52:29, 47.75s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "172it [2:53:20, 48.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "173it [2:53:43, 40.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "174it [2:54:20, 39.81s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "175it [2:55:13, 43.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "176it [2:56:02, 45.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "177it [2:57:38, 60.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "178it [2:59:52, 82.74s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "179it [3:00:52, 75.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "180it [3:01:53, 71.35s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "181it [3:02:52, 67.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "182it [3:03:41, 62.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "183it [3:05:19, 72.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "184it [3:06:08, 65.74s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "185it [3:07:42, 74.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "186it [3:08:22, 63.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "187it [3:09:05, 57.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "188it [3:10:31, 66.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "189it [3:11:02, 55.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "190it [3:11:54, 54.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "191it [3:12:42, 52.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "192it [3:13:46, 56.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "193it [3:14:19, 49.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "194it [3:15:25, 54.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "195it [3:16:31, 57.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "196it [3:17:36, 59.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "197it [3:18:26, 56.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "198it [3:19:12, 53.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "199it [3:20:20, 58.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "200it [3:21:44, 65.66s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "201it [3:22:49, 65.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "202it [3:23:23, 56.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "203it [3:24:33, 60.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "204it [3:25:27, 58.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "205it [3:26:21, 57.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "206it [3:27:30, 60.44s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "weight_ratio = 0.76\n",
    "n_estimators = 120\n",
    "max_depth = 5\n",
    "early_stopping_rounds = 4\n",
    "\n",
    "\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_chain_gb = []\n",
    "\n",
    "X_train, X_val, \n",
    "\n",
    "X = X_train.values\n",
    "X1 = X_val.values\n",
    "X2 = X_test.values\n",
    "for i, y in tqdm(y_train.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    scale_pos_weight = min(round(len(y)/sum(y)-1)*weight_ratio, 8000)\n",
    "    \n",
    "    gbc = XGBClassifier(n_estimators = n_estimators,\n",
    "                        max_depth = max_depth,\n",
    "                        scale_pos_weight  = scale_pos_weight,\n",
    "                        n_jobs = -1,\n",
    "                        random_state=100);\n",
    "    eval_set = [(X1, y_val[i].values)]\n",
    "    gbc.fit(X, y, eval_set=eval_set, eval_metric='logloss', early_stopping_rounds = early_stopping_rounds, verbose=False);\n",
    "    \n",
    "    y_predict = gbc.predict(X).reshape((-1,1))\n",
    "    y_predict1 = gbc.predict(X1).reshape((-1,1))\n",
    "    y_predict2 = gbc.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    \n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "        \n",
    "    X = np.concatenate((X, y_predict), axis=1)\n",
    "    X1 = np.concatenate((X1, y_predict1), axis=1)\n",
    "    X2 = np.concatenate((X2, y_predict2), axis=1)\n",
    "    model_chain_gb.append(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ratio: 0.76  n_estimators: 120  max_depth: 5  early_stopping_rounds: 4\n",
      "Precision: \n",
      "0.9945997928687675\n",
      "Recall: \n",
      "1.0\n",
      "F1: \n",
      "0.9972925861365575\n",
      "\n",
      "Precision: \n",
      "0.7901345291479821\n",
      "Recall: \n",
      "0.25927015891701\n",
      "F1: \n",
      "0.3904276534456016\n"
     ]
    }
   ],
   "source": [
    "print('weight_ratio:',weight_ratio, ' n_estimators:',n_estimators, ' max_depth:', max_depth, ' early_stopping_rounds:', early_stopping_rounds)\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "13518\n",
      "\n",
      "3398\n",
      "1115\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Binary Relavance    \n",
    "#### Model 4.1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [21:37,  6.30s/it]\n"
     ]
    }
   ],
   "source": [
    "weight_ratio = 0.7\n",
    "max_depth = 6\n",
    "\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_list = []\n",
    "\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X = X_train_val.values\n",
    "X2 = X_test.values\n",
    "scaler = StandardScaler()\n",
    "for _, y in tqdm(y_train.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    class_weight = {0:1, 1:min(round(len(y)/sum(y)-1)*weight_ratio, 8000)}\n",
    "    \n",
    "    rfc_2 = RandomForestClassifier(n_estimators = 100,\n",
    "                                  max_depth = max_depth,\n",
    "                                  class_weight = class_weight,\n",
    "                                  n_jobs = 3,\n",
    "                                  random_state=100);\n",
    "    rfc_2.fit(X, y);\n",
    "    \n",
    "    y_predict = rfc_2.predict(X).reshape((-1, 1))\n",
    "    y_predict2 = rfc_2.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    \n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "        \n",
    "    model_list.append(rfc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "# y_train_predict = np.array([])\n",
    "# y_test_predict = np.array([])\n",
    "# X = X_train.values\n",
    "# X2 = X_test.values\n",
    "# for rfc_2 in model_list:\n",
    "#     y_predict = rfc_2.predict(X).reshape((-1, 1))\n",
    "#     y_predict2 = rfc_2.predict(X2).reshape((-1,1))\n",
    "#     if y_test_predict.size == 0:\n",
    "#         y_test_predict = y_predict2\n",
    "#     else:\n",
    "#         y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "#     if y_train_predict.size == 0:\n",
    "#         y_train_predict = y_predict\n",
    "#     else:\n",
    "#         y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ratio:  0.7  max_depth:  6\n",
      "Precision: \n",
      "0.9088415617941271\n",
      "Recall: \n",
      "0.8379323168464113\n",
      "F1: \n",
      "0.871947680043342\n",
      "\n",
      "Precision: \n",
      "0.7120622568093385\n",
      "Recall: \n",
      "0.21542083578575633\n",
      "F1: \n",
      "0.33077270673294173\n"
     ]
    }
   ],
   "source": [
    "# binary relevance\n",
    "print('weight_ratio: ', weight_ratio, ' max_depth: ', max_depth)\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13445\n",
      "12396\n",
      "\n",
      "3398\n",
      "1028\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4.2: Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [2:40:10, 46.65s/it]\n"
     ]
    }
   ],
   "source": [
    "weight_ratio = 0.75\n",
    "n_estimators = 100\n",
    "max_depth = 3\n",
    "early_stopping_rounds = 2\n",
    "\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_list_gb = []\n",
    "\n",
    "X = X_train.values\n",
    "X1 = X_val.values\n",
    "X2 = X_test.values\n",
    "for i, y in tqdm(y_train.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    scale_pos_weight = min(round(len(y)/sum(y)-1)*weight_ratio, 8000)\n",
    "    \n",
    "    gbc = XGBClassifier(n_estimators = n_estimators,\n",
    "                        max_depth = max_depth,\n",
    "                        scale_pos_weight  = scale_pos_weight,\n",
    "                        n_jobs = -1,\n",
    "                        random_state=100);\n",
    "    eval_set = [(X1, y_val[i].values)]\n",
    "    gbc.fit(X, y, eval_set=eval_set, eval_metric='logloss', early_stopping_rounds = early_stopping_rounds, verbose=False);\n",
    "    \n",
    "    y_predict = gbc.predict(X).reshape((-1, 1))\n",
    "    y_predict2 = gbc.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    \n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "\n",
    "    model_list_gb.append(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ratio: 0.75  n_estimators: 100  max_depth: 3  early_stopping_rounds: 2\n",
      "Precision: \n",
      "0.9617240533246094\n",
      "Recall: \n",
      "1.0\n",
      "F1: \n",
      "0.9804886183607104\n",
      "\n",
      "Precision: \n",
      "0.6346685082872928\n",
      "Recall: \n",
      "0.27037363930567815\n",
      "F1: \n",
      "0.37920363111202804\n",
      "\n",
      "11759\n",
      "12227\n",
      "\n",
      "3399\n",
      "1448\n"
     ]
    }
   ],
   "source": [
    "# binary relavance\n",
    "print('weight_ratio:',weight_ratio, ' n_estimators:',n_estimators, ' max_depth:', max_depth, ' early_stopping_rounds:', early_stopping_rounds)\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))\n",
    "print()\n",
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sig_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_0004d9e33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>-0.2140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.2067</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_001897cda</th>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>-1.8820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-1.0310</td>\n",
       "      <td>-1.3670</td>\n",
       "      <td>-0.3690</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_002429b5b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>-0.3390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2261</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>-1.3840</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>-1.9530</td>\n",
       "      <td>-1.0140</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00276f245</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>0.5998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>-0.1784</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.4325</td>\n",
       "      <td>-0.9005</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_0027f1083</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>-0.1580</td>\n",
       "      <td>1.0510</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>1.0900</td>\n",
       "      <td>-0.2962</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cp_type  cp_time  cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "sig_id                                                                    \n",
       "id_0004d9e33      1.0       24      1.0 -0.5458  0.1306 -0.5135  0.4408   \n",
       "id_001897cda      1.0       72      1.0 -0.1829  0.2320  1.2080 -0.4522   \n",
       "id_002429b5b      0.0       24      1.0  0.1852 -0.1404 -0.3911  0.1310   \n",
       "id_00276f245      1.0       24      0.0  0.4828  0.1955  0.3825  0.4244   \n",
       "id_0027f1083      1.0       48      1.0 -0.3979 -1.2680  1.9130  0.2057   \n",
       "\n",
       "                 g-4     g-5     g-6  ...    c-90    c-91    c-92    c-93  \\\n",
       "sig_id                                ...                                   \n",
       "id_0004d9e33  1.5500 -0.1644 -0.2140  ...  0.0981  0.7978 -0.1430 -0.2067   \n",
       "id_001897cda -0.3652 -0.3319 -1.8820  ... -0.1190 -0.1852 -1.0310 -1.3670   \n",
       "id_002429b5b -1.4380  0.2455 -0.3390  ... -0.2261  0.3370 -1.3840  0.8604   \n",
       "id_00276f245 -0.5855 -1.2020  0.5998  ...  0.1260  0.1570 -0.1784 -1.1200   \n",
       "id_0027f1083 -0.5864 -0.0166  0.5128  ...  0.4965  0.7578 -0.1580  1.0510   \n",
       "\n",
       "                c-94    c-95    c-96    c-97    c-98    c-99  \n",
       "sig_id                                                        \n",
       "id_0004d9e33 -0.2303 -0.1193  0.0210 -0.0502  0.1510 -0.7750  \n",
       "id_001897cda -0.3690 -0.5382  0.0359 -0.4764 -1.3810 -0.7300  \n",
       "id_002429b5b -1.9530 -1.0140  0.8662  1.0160  0.4924 -0.1942  \n",
       "id_00276f245 -0.4325 -0.9005  0.8131 -0.1305  0.5645 -0.5809  \n",
       "id_0027f1083  0.5742  1.0900 -0.2962 -0.5313  0.9931  1.8380  \n",
       "\n",
       "[5 rows x 875 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submit = pd.read_csv(data_dir+'test_features.csv', index_col='sig_id')\n",
    "df_submit['cp_type'].replace({'trt_cp':1., 'ctl_vehicle':0.}, inplace=True)\n",
    "df_submit['cp_dose'].replace({'D1':1., 'D2':0.}, inplace=True)\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "y_submit_proba = np.array([])\n",
    "y_submit = np.array([])\n",
    "\n",
    "X_submit = df_submit.values\n",
    "for gbc in model_list_gb:\n",
    "    y_predict_proba = gbc.predict_proba(X_submit)[:,1].reshape(-1,1)\n",
    "    y_predict = gbc.predict(X_submit).reshape(-1,1)\n",
    "    if y_submit_proba.size == 0:\n",
    "        y_submit_proba = y_predict_proba\n",
    "    else:\n",
    "        y_submit_proba = np.concatenate((y_submit_proba, y_predict_proba), axis=1)\n",
    "        \n",
    "    if y_submit.size == 0:\n",
    "        y_submit = y_predict\n",
    "    else:\n",
    "        y_submit = np.concatenate((y_submit, y_predict), axis=1)\n",
    "        \n",
    "y_submit_proba = pd.DataFrame(y_submit_proba)\n",
    "y_submit_proba.index = df_submit.index\n",
    "y_submit_proba.columns = y_train.columns\n",
    "y_submit_proba.reset_index(inplace=True)\n",
    "y_submit_proba.head()\n",
    "\n",
    "y_submit_proba.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4.3 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ratio = 0.5\n",
    "penalty = 'l1'\n",
    "C = 0.5\n",
    "\n",
    "y_train_predict = np.array([])\n",
    "y_test_predict = np.array([])\n",
    "model_list_lr = []\n",
    "\n",
    "X_train_val_norm = pd.concat([X_train_norm, X_val_norm])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "X = X_train_val_norm.values\n",
    "X2 = X_test_norm.values\n",
    "for _, y in tqdm(y_train_val.iteritems()):\n",
    "    # class_weight for each target column\n",
    "    class_weight = {0:1, 1:min(round(len(y)/sum(y)-1)*weight_ratio, 8000)}\n",
    "    \n",
    "    lr_1 = LogisticRegression(penalty = penalty,\n",
    "                            C = C,\n",
    "                            class_weight = class_weight,\n",
    "                            n_jobs = -1,\n",
    "                            solver = 'saga',\n",
    "                            random_state = 100)\n",
    "    lr_1.fit(X, y);\n",
    "    y_predict = lr_1.predict(X).reshape((-1, 1))\n",
    "    y_predict2 = lr_1.predict(X2).reshape((-1,1))\n",
    "    \n",
    "    if y_test_predict.size == 0:\n",
    "        y_test_predict = y_predict2\n",
    "    else:\n",
    "        y_test_predict = np.concatenate((y_test_predict, y_predict2), axis=1)\n",
    "    if y_train_predict.size == 0:\n",
    "        y_train_predict = y_predict\n",
    "    else:\n",
    "        y_train_predict = np.concatenate((y_train_predict, y_predict), axis=1)\n",
    "    model_list_lr.append(lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weight_ratio:',weight_ratio, 'penalty:',penalty, ' C:',C, ')\n",
    "print('Precision: ')\n",
    "print(precision_score(y_train, y_train_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_train, y_train_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_train, y_train_predict))\n",
    "print()\n",
    "print('Precision: ')\n",
    "print(precision_score(y_test, y_test_predict))\n",
    "print('Recall: ')\n",
    "print(recall_score(y_test, y_test_predict))\n",
    "print('F1: ')\n",
    "print(f1_score(y_test, y_test_predict))\n",
    "print()\n",
    "print(y_train.sum().sum())\n",
    "print(y_train_predict.sum())\n",
    "print()\n",
    "print(y_test.sum().sum())\n",
    "print(y_test_predict.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
